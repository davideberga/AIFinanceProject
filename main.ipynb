{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv, set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import math\n",
    "from numpy.random import choice\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Model Packages for reinforcement learning\n",
    "#from keras import layers, models, optimizers\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#from keras import backend as K\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else 'cuda'\n",
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AIFinanceDataloader(Dataset):\n",
    "    def __init__(self, file_csv):\n",
    "        self.dataset = read_csv(file_csv, index_col=0, parse_dates=[0], header=0)\n",
    "        self.dataset.isnull().values.any()\n",
    "        self.dataset=self.dataset.fillna(method='ffill')\n",
    "        # self.dataset['day'] = self.dataset[0].apply(lambda x : x)\n",
    "        self.grouped_by_day = self.dataset.groupby(pd.Grouper(freq='D'))\n",
    "        self.days = []\n",
    "        for date, group in self.grouped_by_day:\n",
    "            if(len(group) > 0):\n",
    "                self.days.append(group)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.days)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day_selected = self.days[idx]\n",
    "        purged = pd.DataFrame(day_selected).drop(columns=[ 'Volume' , 'Open' ])\n",
    "        return purged.to_numpy()\n",
    "\n",
    "train_path = \"lib/data/IVV_1m_training.csv\"\n",
    "validation_path = \"lib/data/IVV_1m_validation.csv\"\n",
    "dataset = AIFinanceDataloader(file_csv = train_path)\n",
    "dataloader_train = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#dataset = AIFinanceDataloader(file_csv = validation_path)\n",
    "#dataloader_val = DataLoader(dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 3)\n",
      "(390, 3)\n"
     ]
    }
   ],
   "source": [
    "# Day zero\n",
    "print(dataset[0].shape)\n",
    "print(dataset[1].shape)\n",
    "\n",
    "feature_size = dataset[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lib.Agent import AgentCNNNetwork\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, feature_size, window_size, is_eval=False, model_name=\"\", dataset_train=None, dataset_val=None):\n",
    "        super(Agent, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.window_size = window_size\n",
    "        self.action_size = 3\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.dataset_train = dataset_train\n",
    "        self.dataset_val = dataset_val\n",
    "        self.model = AgentCNNNetwork(self.feature_size, self.window_size, self.action_size)\n",
    "        self.model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def act(self, state): \n",
    "        #If it is test and self.epsilon is still very high, once the epsilon become low, there are no random\n",
    "        #actions suggested.\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            options = self.model(state.float()).reshape(-1).cpu().numpy()   \n",
    "        \n",
    "        #set_trace()\n",
    "        #action is based on the action that has the highest value from the q-value function.\n",
    "        return np.argmax(options)\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "\n",
    "        self.model.train()\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "        \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward \n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.float()).reshape(-1)).cpu().detach().numpy()   \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    " \n",
    "            target_f = self.model(state.float()).reshape(-1)   \n",
    "            target_f[action] = target\n",
    "\n",
    "            loss = nn.MSELoss()\n",
    "            output = loss(target_f, self.model(state.float()).reshape(-1))\n",
    "            output.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# prints formatted price\n",
    "def formatPrice(n):\n",
    "    return (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
    "\n",
    "# returns an an n-day state representation ending at time t\n",
    "def getState(data, t, n):    \n",
    "    d = t - n + 1\n",
    "\n",
    "    if d>=0:\n",
    "        block = data[d:t + 1, :]\n",
    "    else:\n",
    "        block = torch.cat([ data[0].repeat(-d, 1), data[0:t + 1, :] ])\n",
    "\n",
    "    res = []\n",
    "    for i in range(n - 1):\n",
    "        res.append(block[i + 1].cpu().numpy() - block[i].cpu().numpy())\n",
    "\n",
    "    return torch.transpose(torch.tensor(res).to(device), 0, 1)\n",
    "\n",
    "# Plots the behavior of the output\n",
    "def plot_behavior(data_input, states_buy, states_sell, profit):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='r', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='m', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='k', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title('Total gains: %f'%(profit))\n",
    "    plt.legend()\n",
    "    #plt.savefig('output/'+name+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0/2518\n",
      "torch.Size([390, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.DoubleTensor{[3]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m         plot_behavior(day_episode,states_buy, states_sell, total_profit)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 61\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpReplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal profit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_profit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, BUY trades: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(states_buy)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SELL trades: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(states_sell)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(buy_sell_order)\n",
      "Cell \u001b[0;32mIn[15], line 53\u001b[0m, in \u001b[0;36mAgent.expReplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     52\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \n\u001b[0;32m---> 53\u001b[0m \u001b[43mtarget_f\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m target\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     56\u001b[0m output \u001b[38;5;241m=\u001b[39m loss(target_f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.DoubleTensor{[3]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "\n",
    "window_size = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent(feature_size, window_size, dataset_train=dataloader_train)\n",
    "episode_count = 0\n",
    "\n",
    "\n",
    "for day_episode in dataloader_train:\n",
    "    day_episode = day_episode.squeeze()\n",
    "    print(\"Running episode \" + str(episode_count) + \"/\" + str(len(dataloader_train)))\n",
    "    episode_count += 1\n",
    "    \n",
    "    #set_trace()\n",
    "    total_profit = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "\n",
    "    buy_sell_order = []\n",
    "    print(day_episode.shape)\n",
    "    for t in range(day_episode.size(0)-1):\n",
    "        # print(f\"Step: {t}\")\n",
    "        state = getState(day_episode, t, window_size + 1)\n",
    "        action = agent.act(state)\n",
    "        # sit\n",
    "        next_state = getState(day_episode, t + 1, window_size + 1)\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1: # buy\n",
    "            agent.inventory.append(day_episode[t][2])\n",
    "            states_buy.append(t)\n",
    "            buy_sell_order.append('BUY')\n",
    "            #print(\"Buy: \" + formatPrice(day_episode[t]))\n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "            bought_price = agent.inventory.pop(0)      \n",
    "            reward = day_episode[t][0] - bought_price\n",
    "            total_profit += day_episode[t][2] - bought_price\n",
    "            states_sell.append(t)\n",
    "            buy_sell_order.append('SELL')\n",
    "            \n",
    "        done = True if t == day_episode.size(0) - 1 else False\n",
    "        #appends the details of the state action etc in the memory, which is used further by the exeReply function\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"Total Profit: \" + formatPrice(total_profit))\n",
    "            print(\"--------------------------------\")\n",
    "            #set_trace()\n",
    "            #pd.DataFrame(np.array(agent.memory)).to_csv(\"Agent\"+str(e)+\".csv\")\n",
    "            #Chart to show how the model performs with the stock goin up and down for each \n",
    "            plot_behavior(day_episode,states_buy, states_sell, total_profit)\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.expReplay(batch_size)    \n",
    "\n",
    "    print(f\"Total profit: {total_profit}, BUY trades: {len(states_buy)}, SELL trades: {len(states_sell)}\")\n",
    "    print(buy_sell_order)\n",
    "            \n",
    "\n",
    "    if episode_count % 2 == 0:\n",
    "        print('Ciao sono episodio pari')\n",
    "        #agent.model.save(\"model_ep\" + str(episode_count))\n",
    "        #torch.save(agent.model.state_dict(), \"model_ep\" + str(episode_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
