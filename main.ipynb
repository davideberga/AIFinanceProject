{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from numpy.random import choice\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else 'cuda'\n",
    "#Disable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_path = \"lib/data/IVV_1m_training.csv\"\n",
    "validation_path = \"lib/data/IVV_1m_validation.csv\"\n",
    "# dataset = AIFinanceDataloader(file_csv = train_path)\n",
    "# dataloader_train = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#dataset = AIFinanceDataloader(file_csv = validation_path)\n",
    "#dataloader_val = DataLoader(dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day zero\n",
    "# print(dataset[0].shape)\n",
    "# print(dataset[1].shape)\n",
    "\n",
    "# feature_size = dataset[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lib.AgentNetworks import AgentCNNNetwork, AgentLSTMNetwork, AgentGRUNetwork\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, feature_size, window_size, is_eval=False, model_name=\"\"):\n",
    "        super(Agent, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.window_size = window_size\n",
    "        self.action_size = 3\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.model = AgentGRUNetwork(self.feature_size, self.window_size, self.action_size, device, is_eval)\n",
    "        self.model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def act(self, state): \n",
    "        #If it is test and self.epsilon is still very high, once the epsilon become low, there are no random\n",
    "        #actions suggested.\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            options = self.model(state.float()).reshape(-1).cpu().numpy()   \n",
    "        \n",
    "        #set_trace()\n",
    "        #action is based on the action that has the highest value from the q-value function.\n",
    "        return np.argmax(options)\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "\n",
    "        self.model.train()\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "        \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward \n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.float()).reshape(-1)).cpu().detach().numpy()   \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    " \n",
    "            target_f = self.model(state.float()).reshape(-1)   \n",
    "            target_f[action] = target\n",
    "\n",
    "            loss = nn.MSELoss()\n",
    "            output = loss(target_f, self.model(state.float()).reshape(-1))\n",
    "            output.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# prints formatted price\n",
    "def formatPrice(n):\n",
    "    return (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
    "\n",
    "# Plots the behavior of the output\n",
    "def plot_behavior(data_input, states_buy, states_sell, profit):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='r', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='m', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='k', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title('Total gains: %f'%(profit))\n",
    "    plt.legend()\n",
    "    #plt.savefig('output/'+name+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0/2518\n",
      "Total profit: 0, BUY trades: 0, SELL trades: 0\n",
      "[]\n",
      "Running episode 1/2518\n",
      "Total profit: 0, BUY trades: 0, SELL trades: 0\n",
      "[]\n",
      "Running episode 2/2518\n",
      "Total profit: 0, BUY trades: 0, SELL trades: 0\n",
      "[]\n",
      "Running episode 3/2518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 30\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpReplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     32\u001b[0m     observation \u001b[38;5;241m=\u001b[39m next_observation\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# plot_behavior(day_episode, states_buy, states_sell, total_profit)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mAgent.expReplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m target \u001b[38;5;241m=\u001b[39m reward \n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 47\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()   \n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     51\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from lib.IVVEnvironment import IVVEnvironment\n",
    "\n",
    "window_size = 10\n",
    "batch_size = 16\n",
    "feature_size = 3\n",
    "seed = 9\n",
    "\n",
    "agent = Agent(feature_size, window_size)\n",
    "train_environment = IVVEnvironment(train_path, seed=seed, device=device, trading_cost=1e-3)\n",
    "\n",
    "episode_count = 0\n",
    "while(train_environment.there_is_another_episode()):\n",
    "    print(\"Running episode \" + str(episode_count) + \"/\" + str(train_environment.num_of_ep()))\n",
    "\n",
    "    # Reset the environment and obtain the initial observation\n",
    "    observation = train_environment.reset()\n",
    "    info = {}\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.act(observation)\n",
    "        next_observation, reward, done, info = train_environment.step(action)\n",
    "\n",
    "        agent.memory.append((observation, action, reward, next_observation, done))\n",
    "\n",
    "        if done: break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.expReplay(batch_size) \n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "    # plot_behavior(day_episode, states_buy, states_sell, total_profit)\n",
    "    print(f\"Total profit: {info['total_profit']}, BUY trades: {len(info['when_bought'])}, SELL trades: {len(info['when_sold'])}\")\n",
    "    print(info['buy_sell_order'])\n",
    "\n",
    "    episode_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
